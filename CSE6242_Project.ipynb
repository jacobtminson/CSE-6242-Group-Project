{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is an outline for methods.\n",
    "\n",
    "## I `highlighted` for parts that we need to discuss/agree as a `TEAM`, `DATA` parts that need to be done via data-preprocessing, or any updates in general.\n",
    "## FYI, I also used `@name` to refer to certain team members."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\tData Collection\n",
    "### 2.\tFeature Selection:\n",
    "Identify the most relevant features from the dataset.\\\n",
    "Graduation Rate, Net Price, Acceptance Rate, Salary, Debt, and Size. \n",
    "\n",
    "### 3.\tData Preprocessing:\n",
    "**`DATA:` these parts should be addressed via pre-processing so that data will be ready for modeling & UI with the right variables**\\\n",
    "`@Jacob @Nicolas, you can refer to this file to decide which variable that you guys want to keep during preprocessing)`\n",
    "\n",
    "**`TEAM:` these parts should be addressed by the whole team to decide which approach to take**\n",
    "\n",
    "#### Modeling\n",
    "For modeling, need to merge MERGED & Recent Cohorts Institution to make ranked_list.\\\n",
    "For modeling, we don't need normalization.\\\n",
    "For the model, we need the **selected_features**\n",
    "\n",
    "**selected_features & Variable names:**\n",
    "- Graduation Rate: C100_4 (Completion rate for first-time, full-time students at four-year institutions **(100% of expected time to completion)**) \n",
    "- Net price: NPT4_PUB, NPT4_PRIV (Average net price for public/private)\n",
    "    - `DATA:` Merge these two columns into one; we ignore public/private\n",
    "- Acceptance Rate: ADM_RATE \n",
    "- Salary: MD_EARN_WNE_1YR (Median earnings of graduates working and not enrolled 1 year after completing)\n",
    "    - MDEARN_ALL (Overall median earnings of students working and not enrolled 10 years after entry)\n",
    "    - `NEED TO:` how to handle # years after completly/entry/etc..\n",
    "- Debt: GRAD_DEBT_MDN_SUPP (Median Total Debt After Graduation for Loans Taken Out at This School) OR Cumulative loan debt at the @@th percentile\n",
    "    - `NEED TO:` have to decide which variable to choose - or how to handle percentile..\n",
    "- Size: GRADS (# of grad students), D_PCTPELL_PCTFLOAN (# of undergrad, denominator receiving pell grant)\n",
    "    - `DATA:` A new column of total # of students (GRADS + D_PCTPELL_PCTFLOAN)\n",
    "\n",
    "#### UI\n",
    "`@Clara @Satish, please let me know any concerns or parts to edit/update`\n",
    "\n",
    "For UI, also need to use FoS and Recent Fos along with the ranked_list (for filtering Majors)\\\n",
    "We need annualized metrics that show increase/decrease over years (school_history)\\\n",
    "For UI, we need **filtering_options** (user input) and **school_history** (that shows increase/decrease of rates for each school).\n",
    "\n",
    "**Filtering_options & Variable names:**\n",
    "- Test scores (required): SATVR25 ACTCM25,,,  \n",
    "    - `NEED TO:` how to handle percentile \n",
    "- Location (required): STABBR (abbreviation of state)\n",
    "- Major (required): CIPDESC (in FoS and Recent FoS files); will not use detailed version of the majors; show dropdown and user can click on the display of majors\n",
    "- `NEED TO:` Other than these 3 required filters, we should pick up more optional filters (debt, net price, salary, public/private, women only, family income FAMINC etc)\n",
    "    - We want to use as much as we can. Will need to finalize via exploring data\n",
    "    - `NEED TO:` All team members should explore through the variables and share ideas, then we can have our final optional filters\n",
    "\n",
    "**School_history & Variable names:**\n",
    "- Admission rate: ADM_RATE  \n",
    "- Net price: NPT4_PUB, NPT4_PRIV \n",
    "    - `DATA:` Merge these two columns into one; we ignore public/private (mentioned above)\n",
    "- Debt\n",
    "    - `NEED TO:` how to handle percentile..\n",
    "\n",
    "\n",
    "### 4.\tData Analysis:\n",
    "Use statistical analysis and data visualization techniques to gain insights into the relationships between different features and university quality.\\ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\tScore Calculation / Ranking:\n",
    "Develop a scoring algorithm that combines the selected features to create a university score. We can use various methods such as weighted averages or machine learning models to do this. For example, we might want to give more weight to Graduation Rate and Salary while considering other factors with different weights.\n",
    "\n",
    "**Weighted Average:** \\\n",
    "Assign a weight to each feature, reflecting its importance, and calculate the weighted sum of the features for each university. For example, we might assign higher weights to Graduation Rate and Salary, indicating their greater importance in our ranking.\n",
    "\n",
    "In this step, we need to develop a scoring algorithm that assigns a score to each university based on the selected features. The scoring algorithm depends on our project's objectives and the weight we assign to each feature. Here's an example using a weighted average approach:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Weighted Average Score Calculation\n",
    "\n",
    "# selected features to compute score\n",
    "# data: merged data of different 4 files (pre-processed)\n",
    "selected_features = data[\"Graduation Rate\", \"Average Annual Cost\", \"Acceptance Rate\", \"Salary\", \"Debt\", \"Size\"]\n",
    "\n",
    "# Assign weights to each feature based on their importance\n",
    "weights = {\n",
    "    \"Graduation Rate\": 0.2,\n",
    "    \"Average Annual Cost\": 0.15,\n",
    "    \"Acceptance Rate\": 0.1,\n",
    "    \"Salary\": 0.2,\n",
    "    \"Debt\": 0.15,\n",
    "    \"Distance\": 0.1,\n",
    "    \"Size\": 0.1\n",
    "}\n",
    "\n",
    "# Calculate the university score\n",
    "data['University Score'] = (selected_features * weights).sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can adjust the weights to reflect our priorities. The total weight should sum to 1. This approach provides a single score for each university based on the weighted average of features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\tModel:\n",
    "We can build a predictive model to rank universities. Regression models, classification models, or ensemble methods can be used to predict the university score based on historical data. We can build a regression model that predicts the university score directly. Regression models like linear regression, decision trees, or even more advanced methods like random forests or gradient boosting can be used. In this approach, the model learns the relationship between the features and the score from historical data.\n",
    "\n",
    "\n",
    "**Feature Engineering? (similar to 'score'):**\\\n",
    "Create any additional features or modify existing features that may improve the model's performance. For example, we can create new features like \"Student-to-Faculty Ratio\" or \"Student Satisfaction Index\" by combining existing data.\n",
    "\n",
    "**Model - Gradient Boosting Regressors (ensemble method):** \\\n",
    "Ensemble methods are a powerful approach to improving the accuracy and robustness of regression models; often outperform individual models, providing better predictive accuracy. \\\n",
    "Gradient boosting is known for its high predictive accuracy. It can capture complex relationships between multiple features and university scores.\\\n",
    "They work by training multiple weak learners sequentially and combining their predictions. They are powerful and can capture complex non-linear relationships.\n",
    "\n",
    "Gradient boosting is robust against outliers and can handle noisy data effectively.\n",
    "We can easily fine-tune hyperparameters to optimize the model's performance and control overfitting.\n",
    "Gradient boosting is efficient and can handle large datasets.\n",
    "\n",
    "**Algorithms for GB:** XGBoost, LightGBM, and CatBoost (for Categorical features,,)\n",
    "\n",
    "**LightGBM** is known for its speed and efficiency, making it a good choice for large datasets.\\\n",
    "It uses a leaf-wise tree growth strategy that can lead to faster training times.\\\n",
    "XGBoost and CatBoost are also efficient, but LightGBM is often faster in terms of training speed.\\\n",
    "LightGBM is designed to be memory-efficient, which is beneficial when dealing with large datasets. It optimizes memory usage during training.\n",
    "\n",
    "While LightGBM is a strong option, it's still a good practice to experiment with different models and fine-tune hyperparameters to ensure that it performs well on your specific dataset. However, given the size of our data (millions of rows), LightGBM's efficiency and speed make it a sensible choice.\n",
    "\n",
    "** Split our dataset into a training set and a testing set to train and evaluate our model. (70-30 or **80-20**.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# data loaded into X and y\n",
    "# X contains features (independent variables)\n",
    "\n",
    "# y contains target variable (university scores)\n",
    "y = data['University Score']\n",
    "\n",
    "# Split the data into training and testing sets (adjust the test_size as needed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# You can also specify a random_state to ensure reproducibility by setting it to a fixed value.\n",
    "\n",
    "# Now, X_train and y_train are your training data, and X_test and y_test are your testing data.\n",
    "\n",
    "\n",
    "# Create a LightGBM dataset\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "# Define model parameters\n",
    "params = {\n",
    "    'objective': 'regression',  # Regression task\n",
    "    'boosting_type': 'gbdt',    # Gradient Boosting Decision Tree\n",
    "    'metric': {'l2', 'l1'},    # MAE (mean absolute error) and MSE (mean squared error)\n",
    "    'num_leaves': 31,           # Maximum number of leaves in one tree **\n",
    "    'learning_rate': 0.05,      # learning rate **\n",
    "    'feature_fraction': 0.9,   # Percentage of features to use per tree **\n",
    "    'bagging_fraction': 0.8,   # Percentage of data to bag per iteration **\n",
    "    'bagging_freq': 5,         # Bagging frequency **\n",
    "    'verbose': 0               # No output during training\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "num_boost_round = 100  # Number of boosting rounds (adjust this) **\n",
    "lgb_model = lgb.train(params, train_data, num_boost_round=num_boost_round)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter Tuning:** \\\n",
    "Optimize the model's hyperparameters to improve its performance. This may involve **grid search**, random search, or other hyperparameter optimization techniques.\n",
    "\n",
    "We can build a predictive model to rank universities based on their scores. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50, 100],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'feature_fraction': [0.8, 0.9, 1.0],\n",
    "    'bagging_fraction': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "# Create a Grid Search model\n",
    "grid_search = GridSearchCV(lgb_model, param_grid, cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit the search on your training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.\tModel Evaluation:\n",
    "Evaluate the model's performance using the testing dataset. Common regression evaluation metrics include Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R-squared (R2). The choice of metric depends on our project's goals. \n",
    "\n",
    "Calculate the Mean Absolute Error (MAE) and Mean Squared Error (MSE) to measure the average and squared differences between the predicted scores and the actual scores. Lower values indicate a better model.\\\n",
    "RMSE is the square root of MSE and provides a more interpretable error metric in the same unit as our scores.\\\n",
    "R-squared measures the proportion of variance in the scores explained by the model. A higher R2 value indicates a better fit.\n",
    "\n",
    "**Cross-Validation:** \\\n",
    "To ensure that our model generalizes well to new data, perform cross-validation, such as k-fold cross-validation or `Monte Carlo`. It divides the dataset into k subsets, trains and tests the model k times, and computes the average performance metrics. \n",
    "\n",
    "Assess the model's bias-variance trade-off. A model with high bias underfits the data, while a model with high variance overfits. We want to strike a balance.\n",
    "\n",
    "**Visualizations:** \\\n",
    "Create visualizations like scatter plots to compare predicted scores with actual scores, helping to identify areas where the model might be performing poorly.\n",
    "\n",
    "**Hyperparameter Tuning - again (hoping this would not happen):** \\\n",
    "If the model performance is not satisfactory, we may need to revisit feature selection, feature engineering, and model selection or consider collecting additional data.\n",
    "\n",
    "`In case the results are pretty enough, we can add that if GB doesn't behave well, we might consider running a new model in random forest to the progress report in this part.`\\\n",
    "`-> mentioned below as well, but will think about this while writing the report to finalize`\n",
    "\n",
    "Once we've built a model, we need to evaluate its performance. Here's how to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = lgb_model.predict(X_test)  # Use the best model using the tuned parameters from grid search\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R-squared (R2): {r2}\")\n",
    "\n",
    "# Create a DataFrame to store the universities and their predicted scores\n",
    "ranking_df = pd.DataFrame({'University': X_test['University'], 'Predicted_Score': y_pred})\n",
    "\n",
    "# Sort the DataFrame by predicted scores in descending order to get the ranked list\n",
    "ranked_universities = ranking_df.sort_values(by='Predicted_Score', ascending=False)\n",
    "\n",
    "# Display the ranked universities\n",
    "print(ranked_universities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we calculate common regression metrics such as MAE, MSE, RMSE, and R2. These metrics help assess how well our model performs at predicting university scores. Lower MAE and RMSE values and higher R2 values indicate better model performance.\n",
    "\n",
    "These are just simplified code snippets to give us an idea of the process. We'll likely need to fine-tune our model, use cross-validation, and conduct a more in-depth analysis of our data to ensure the accuracy of our university rankings. Additionally, we may want to create visualizations to communicate the results effectively.\n",
    "\n",
    "**Post-ranking model:**\\\n",
    "Once the model is good to go, we will have the ranking list of millions rows. Finally, we will filter the ranking list upon the user's filting options.\n",
    "\n",
    "`For sake of the outputs on the UI, we need to have a method that shows a single (selected by user from top 10) school's name, location; admission rate, net price, debt over years (median/mean).`\n",
    "\n",
    "#### `Update`\n",
    "`actually how about having baseline models along with ensemble methods in the beginning and then compare to pick the best??` \\\n",
    "`-> will think about this more while writing report (mentioned above)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.\tVisualization:\n",
    "Create visual representations of the university scores and rankings. This can include bar charts, scatter plots, and interactive dashboards for better data communication.\n",
    "\n",
    "### 9.\tUser Interface:\n",
    "If our goal is to make the results accessible to a wider audience, consider building a user-friendly interface or a website where users can input their preferences and see university rankings based on their criteria."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
